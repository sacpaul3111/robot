---
#############################################################################
# Ansible Playbook: Robot Framework Test Execution
# Description: Execute all Robot Framework tests with pass/fail summary
#
# USAGE:
#   # Method 1: With Ansible Vault (recommended for production)
#   ansible-playbook run_tests.yml \
#     -e TargetHostname=<hostname> \
#     -e @/etc/ansible/vault/robot_credentials.yml \
#     --vault-password-file=/etc/ansible/vault/.vault_pass
#
#   # Method 2: With environment variables (development)
#   source credentials.env
#   ansible-playbook run_tests.yml -e TargetHostname=<hostname>
#
#   # Method 3: Using wrapper script (easiest)
#   ./run_tests_vault.sh <hostname>
#
# For detailed documentation, see: docs/ENVIRONMENT_VARIABLES_APPROACH.md
#############################################################################

- name: Execute Robot Framework Tests
  hosts: localhost
  connection: local
  gather_facts: yes
  vars:
    # Default values - can be overridden with -e flag
    robot_project_dir: "{{ playbook_dir }}/.."
    robot_output_dir: "results"

    # TARGET_HOSTNAME - Primary variable used by Robot Framework tests
    # Accept TargetHostname (from Itential) and map to TARGET_HOSTNAME
    target_hostname: "{{ TargetHostname | default('') }}"

    # All credentials are provided via environment variables
    # No need to define them here - they are passed directly to Robot Framework
    # via the environment block in execute_test_suite.yml

    fail_on_test_errors: false

  pre_tasks:
    - name: Display banner
      debug:
        msg:
          - "========================================================"
          - "   GSA Itential Robot Framework - Test Execution"
          - "========================================================"
          - "Project Directory: {{ robot_project_dir }}"
          - "Target Hostname:   {{ target_hostname }}"
          - "Output Directory:  {{ robot_output_dir }}"
          - "Operating System:  {{ ansible_distribution }} {{ ansible_distribution_version }}"
          - "========================================================"

    #########################################################################
    # PREREQUISITE CHECKS AND INSTALLATION
    #########################################################################

    - name: Check if Python 3 is installed
      command: python3 --version
      register: python3_check
      ignore_errors: yes
      changed_when: false

    - name: Check if pip3 is installed
      command: pip3 --version
      register: pip3_check
      ignore_errors: yes
      changed_when: false

    - name: Check if git is installed
      command: git --version
      register: git_check
      ignore_errors: yes
      changed_when: false

    - name: Display prerequisite check results
      debug:
        msg:
          - "Prerequisite Check Results:"
          - "  Python 3: {{ 'Installed' if python3_check.rc == 0 else 'NOT INSTALLED' }}"
          - "  pip3:     {{ 'Installed' if pip3_check.rc == 0 else 'NOT INSTALLED' }}"
          - "  git:      {{ 'Installed' if git_check.rc == 0 else 'NOT INSTALLED' }}"

    - name: Install system packages (RHEL/CentOS/Rocky)
      become: yes
      yum:
        name:
          - python3
          - python3-pip
          - python3-devel
          - git
          - gcc
          - libxml2-devel
          - libxslt-devel
          - libffi-devel
          - openssl-devel
          - sshpass
        state: present
      when:
        - ansible_os_family == 'RedHat'
        - python3_check.rc != 0 or pip3_check.rc != 0 or git_check.rc != 0
      ignore_errors: yes

    - name: Install system packages (Ubuntu/Debian)
      become: yes
      apt:
        name:
          - python3
          - python3-pip
          - python3-dev
          - git
          - gcc
          - libxml2-dev
          - libxslt1-dev
          - libffi-dev
          - libssl-dev
          - sshpass
        state: present
        update_cache: yes
      when:
        - ansible_os_family == 'Debian'
        - python3_check.rc != 0 or pip3_check.rc != 0 or git_check.rc != 0
      ignore_errors: yes

    - name: Upgrade pip to latest version
      pip:
        name: pip
        executable: pip3
        state: latest
        extra_args: --user
      ignore_errors: yes

    - name: Check if Robot Framework is installed
      command: python3 -m robot --version
      register: robot_check
      ignore_errors: yes
      changed_when: false

    - name: Check if SSHLibrary is installed
      command: python3 -c "import SSHLibrary"
      register: sshlibrary_check
      ignore_errors: yes
      changed_when: false

    - name: Install Robot Framework and required libraries
      pip:
        name:
          - robotframework>=3.2.2
          - robotframework-sshlibrary>=3.7.0
          - robotframework-seleniumlibrary>=5.1.3
          - requests>=2.27.1
          - paramiko>=2.9.2
          - cryptography>=36.0.1
          - lxml>=4.7.1
          - openpyxl>=3.0.9
          - pandas>=1.3.5
        executable: pip3
        state: present
        extra_args: --user
      when: robot_check.rc != 0 or sshlibrary_check.rc != 0
      ignore_errors: yes

    - name: Verify Python 3 is available after installation
      command: python3 --version
      register: python3_verify
      changed_when: false
      failed_when: python3_verify.rc != 0

    - name: Verify Robot Framework is available after installation
      command: python3 -m robot --version
      register: robot_verify
      changed_when: false
      failed_when: robot_verify.rc != 0

    - name: Display installed versions
      debug:
        msg:
          - "Installed Software Versions:"
          - "  {{ python3_verify.stdout }}"
          - "  {{ robot_verify.stdout }}"

    #########################################################################
    # VALIDATION
    #########################################################################

    - name: Validate target hostname is provided
      fail:
        msg: |
          ERROR: Target hostname is required!
          Usage: ansible-playbook run_tests.yml -e TargetHostname=<hostname>
          Example: ansible-playbook run_tests.yml -e TargetHostname=alhxvdvitap01
      when: target_hostname == ''

    - name: Validate SSH credentials are provided via environment
      fail:
        msg: |
          ERROR: SSH credentials required!
          Please ensure environment variables are set:
            export SSH_USERNAME=<user>
            export SSH_PASSWORD=<pass>

          Or source credentials file:
            source credentials.env

          Or use wrapper script:
            ./run_tests.sh {{ target_hostname }}
      when: lookup('env', 'SSH_USERNAME') == '' or lookup('env', 'SSH_PASSWORD') == ''

    - name: Create project directory if it doesn't exist
      file:
        path: "{{ robot_project_dir }}"
        state: directory
        mode: '0755'

  tasks:
    - name: Set Python executable to system python
      set_fact:
        python_executable: "python3"

    - name: Display Python executable being used
      debug:
        msg: "Using Python: {{ python_executable }}"

    - name: Check if tests directory exists
      stat:
        path: "{{ robot_project_dir }}/tests"
      register: tests_dir_stat

    - name: Fail if tests directory does not exist
      fail:
        msg: "Tests directory not found: {{ robot_project_dir }}/tests"
      when: not tests_dir_stat.stat.exists

    - name: Create output directory
      file:
        path: "{{ robot_project_dir }}/{{ robot_output_dir }}"
        state: directory
        mode: '0755'

    - name: Find all test suite directories
      find:
        paths: "{{ robot_project_dir }}/tests"
        file_type: directory
        patterns: 'test*_*'
      register: test_directories

    - name: Display found test suites
      debug:
        msg: "Found {{ test_directories.files | length }} test suite(s)"

    - name: Initialize test results tracking
      set_fact:
        test_results: []
        all_output_files: []
        total_tests: 0
        passed_tests: 0
        failed_tests: 0

    - name: Execute each test suite
      include_tasks: execute_test_suite.yml
      loop: "{{ test_directories.files | sort(attribute='path') }}"
      loop_control:
        loop_var: test_dir

    - name: Create consolidated reports using rebot
      command: >
        {{ python_executable }} -m robot.rebot
        --outputdir {{ robot_project_dir }}/{{ robot_output_dir }}
        --output all_output.xml
        --log all_log.html
        --report all_report.html
        {{ all_output_files | join(' ') }}
      args:
        chdir: "{{ robot_project_dir }}"
      register: rebot_result
      ignore_errors: yes
      when: all_output_files | length > 0

    - name: Check if consolidated report was created
      stat:
        path: "{{ robot_project_dir }}/{{ robot_output_dir }}/all_report.html"
      register: consolidated_report_stat

    - name: Parse consolidated output.xml for final results
      xml:
        path: "{{ robot_project_dir }}/{{ robot_output_dir }}/all_output.xml"
        xpath: /robot/statistics/total/stat
        content: attribute
      register: final_statistics
      when: consolidated_report_stat.stat.exists
      ignore_errors: yes

    - name: Extract pass/fail counts from consolidated results
      set_fact:
        total_tests_final: "{{ final_statistics.matches[0].stat.pass | default('0') | int + final_statistics.matches[0].stat.fail | default('0') | int }}"
        passed_tests_final: "{{ final_statistics.matches[0].stat.pass | default('0') | int }}"
        failed_tests_final: "{{ final_statistics.matches[0].stat.fail | default('0') | int }}"
      when:
        - consolidated_report_stat.stat.exists
        - final_statistics.matches is defined
        - final_statistics.matches | length > 0
      ignore_errors: yes

    - name: Set final test status
      set_fact:
        all_tests_passed: "{{ (failed_tests_final | default(failed_tests) | int) == 0 and (total_tests_final | default(total_tests) | int) > 0 }}"
        results_directory: "{{ robot_project_dir }}/{{ robot_output_dir }}"
        consolidated_log: "{{ robot_project_dir }}/{{ robot_output_dir }}/all_log.html"
        consolidated_report: "{{ robot_project_dir }}/{{ robot_output_dir }}/all_report.html"
        total_count: "{{ total_tests_final | default(total_tests) }}"
        passed_count: "{{ passed_tests_final | default(passed_tests) }}"
        failed_count: "{{ failed_tests_final | default(failed_tests) }}"
        nexus_artifacts_url: "https://nexus.company.com/repository/robot-framework-artifacts/{{ target_hostname }}/{{ ansible_date_time.iso8601 | regex_replace('[:]', '-') }}"

  post_tasks:
    - name: Display test execution summary
      debug:
        msg:
          - "========================================================"
          - "           TEST EXECUTION SUMMARY"
          - "========================================================"
          - "Total Tests:    {{ total_count }}"
          - "Passed Tests:   {{ passed_count }}"
          - "Failed Tests:   {{ failed_count }}"
          - "========================================================"
          - "Overall Status: {{ 'PASSED ✅' if all_tests_passed else 'FAILED ❌' }}"
          - "========================================================"
          - ""
          - "Results Location:"
          - "  Directory:    {{ results_directory }}"
          - "  Log:          {{ consolidated_log }}"
          - "  Report:       {{ consolidated_report }}"
          - ""
          - "Artifacts URL (Nexus OSS):"
          - "  {{ nexus_artifacts_url }}"
          - "========================================================"

    - name: Display individual test suite results
      debug:
        msg: "{{ test_results }}"
      when: test_results | length > 0

    - name: Save test summary to file
      copy:
        content: |
          ========================================================
          GSA Itential Robot Framework - Test Execution Summary
          ========================================================
          Execution Date: {{ ansible_date_time.iso8601 }}
          Target Hostname: {{ target_hostname }}

          ========================================================
          RESULTS SUMMARY
          ========================================================
          Total Tests:    {{ total_count }}
          Passed Tests:   {{ passed_count }}
          Failed Tests:   {{ failed_count }}
          Overall Status: {{ 'PASSED' if all_tests_passed else 'FAILED' }}

          ========================================================
          INDIVIDUAL TEST SUITES
          ========================================================
          {% for result in test_results %}
          {{ result }}
          {% endfor %}

          ========================================================
          RESULTS LOCATION
          ========================================================
          Directory:  {{ results_directory }}
          Log:        {{ consolidated_log }}
          Report:     {{ consolidated_report }}

          ========================================================
          ARTIFACTS URL (NEXUS OSS)
          ========================================================
          {{ nexus_artifacts_url }}
          ========================================================
        dest: "{{ robot_project_dir }}/{{ robot_output_dir }}/TEST_SUMMARY.txt"
        mode: '0644'

    - name: Display summary file location
      debug:
        msg: "Test summary saved to: {{ robot_project_dir }}/{{ robot_output_dir }}/TEST_SUMMARY.txt"

    - name: Fail playbook if tests failed and fail_on_test_errors is true
      fail:
        msg: |
          ❌ TEST EXECUTION FAILED ❌
          {{ failed_count }} test(s) failed
          Check reports at: {{ results_directory }}
      when:
        - not all_tests_passed
        - fail_on_test_errors | bool
